{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.distribute.Strategy\n",
    "## High level APIs\n",
    "## Custom training loops\n",
    "## Tensorflow 2 eager mode & graph mode\n",
    "## Supported on multiple configurations\n",
    "## Convenient to use with little to no code changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commonly used terms\n",
    "## Device : CPU, GPU, TPU\n",
    "## Replica : copy of the models variables on several devices\n",
    "## Worker : software running on a device that's dedicated to traning the replica\n",
    "## Mirrored variable : some variables that you want to be in sync across all devices.  \n",
    "# ###                   So The variables within these models that we want to keep in sync across all of the devices we'll call mirrored variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifying strategies\n",
    "## Hardware platforms : single - machine multi - device, Multi - machine\n",
    "## Training : \n",
    "# ### Synchronous (all - reduce) : all workers train over different slices of input data in sync with each other\n",
    "#                                  they'll aggregate gradients at each step using an all-reduce algorithm\n",
    "# ### Asynchronous (Parameter Server) : all workers are independently training over the input data \n",
    "#                                        and they'are updating that variables asynchronously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MirrorStrategy: Single machine multi GPU, creates a replica per GPU, Each variable is mirrored, All-reduce across devices\n",
    "# TPUStrategy : Same as MirroredStrategy, All-reduce across TPU cores\n",
    "# MultiWorkerMirroredStrategy : Multi machine multi GPU, Replicates variables per device across workers, All-reduce based on hardware, network topology, tensor sizes\n",
    "# CentralStorageStrategy : Variables are not mirrored (Instead placed on the CPU), Done in-memory on a device\n",
    "# ParameterServerStragey : Some machines designated as workers, Some others as parameter servers\n",
    "# DefaultStrategy : Simple Passthrough\n",
    "# OneDeviceStrategy : Single device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mirrored Strategy (Code Changes)\n",
    "## Model declaration\n",
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import os \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets , info = tfds.load(name = 'mnist', with_info = True, as_supervised = True)\n",
    "mnist_train, mnist_test = datasets['train'], datasets['test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-07 11:57:26.541045: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-01-07 11:57:26.541217: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-01-07 11:57:26.542138: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "# model = tf.keras.Sequential([ \n",
    "#     tf.keras.layers.Conv2D(32, 3, activation = 'relu', input_shape = (28, 28, 1)), \n",
    "#     tf.keras.layers.MaxPooling2D(), \n",
    "#     tf.keras.layers.Flatten(), \n",
    "#     tf.keras.layers.Dense(64, activation = 'relu'), \n",
    "#     tf.keras.layers.Dense(10)\n",
    "# ])\n",
    "\n",
    "# model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True), \n",
    "# optimizer = tf.keras.optimizers.Adam(), \n",
    "# metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def scale(image, label): \n",
    "#     image = tf.cast(image, tf.float32)\n",
    "#     image /= 255.0 \n",
    "#     return image\n",
    "\n",
    "# num_train_examples = info.splits['train'].num_examples\n",
    "# num_test_examples = info.splits['test'].num_examples\n",
    "\n",
    "# BUFFER_SIZE = 10000\n",
    "# BATCH_SIZE = 64\n",
    "\n",
    "# train_dataset = mnist_train.map(scale).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "# eval_dataset = mnist_test.map(scale).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distributed Training : Mirrored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print(strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(image, label): \n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 255\n",
    "    return image, label\n",
    "\n",
    "num_train_examples = info.splits['train'].num_examples\n",
    "num_test_examples = info.splits['test'].num_examples\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE_PER_REPLICA = 64\n",
    "BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n",
    "\n",
    "train_dataset = mnist_train.map(scale).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "eval_dataset = mnist_test.map(scale).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    model = tf.keras.Sequential([ \n",
    "        tf.keras.layers.Conv2D(32, 3, activation = 'relu', input_shape = (28, 28, 1)), \n",
    "        tf.keras.layers.MaxPooling2D(), \n",
    "        tf.keras.layers.Flatten(), \n",
    "        tf.keras.layers.Dense(64, activation = 'relu'), \n",
    "        tf.keras.layers.Dense(10, activation = 'softmax')\n",
    "    ])\n",
    "\n",
    "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True), \n",
    "optimizer = tf.keras.optimizers.Adam(), \n",
    "metrics = ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-07 12:36:59.576722: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "938/938 [==============================] - 46s 49ms/step - loss: 0.3979 - accuracy: 0.8851\n",
      "Epoch 2/12\n",
      "938/938 [==============================] - 56s 60ms/step - loss: 0.0715 - accuracy: 0.9796\n",
      "Epoch 3/12\n",
      "938/938 [==============================] - 51s 54ms/step - loss: 0.0510 - accuracy: 0.9846\n",
      "Epoch 4/12\n",
      "938/938 [==============================] - 52s 56ms/step - loss: 0.0386 - accuracy: 0.9882\n",
      "Epoch 5/12\n",
      "938/938 [==============================] - 46s 50ms/step - loss: 0.0286 - accuracy: 0.9910\n",
      "Epoch 6/12\n",
      "195/938 [=====>........................] - ETA: 42s - loss: 0.0300 - accuracy: 0.9915"
     ]
    }
   ],
   "source": [
    "model.fit(train_dataset, epochs=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training across local GPUs\n",
    "## Each variable in the model is mirrored across all replicas\n",
    "## Variables are treated as MirroredVariable\n",
    "## Synchronization done with NVIDIA NCCL"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "366f001366255956e9ab068c1919ba0fdaf430e3ffbc7099a0724add2b197f30"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('implementations': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
